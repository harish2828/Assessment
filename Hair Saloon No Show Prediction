{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":62249,"sourceType":"datasetVersion","datasetId":40069}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##Import necessary libraries\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:39.557896Z","iopub.execute_input":"2025-06-11T04:51:39.558238Z","iopub.status.idle":"2025-06-11T04:51:39.563904Z","shell.execute_reply.started":"2025-06-11T04:51:39.558215Z","shell.execute_reply":"2025-06-11T04:51:39.563029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 1: Get the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hair-salon-no-show-data-set/hair_salon_no_show_wrangled_df.csv')\ndf.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:39.567693Z","iopub.execute_input":"2025-06-11T04:51:39.568646Z","iopub.status.idle":"2025-06-11T04:51:39.608999Z","shell.execute_reply.started":"2025-06-11T04:51:39.568625Z","shell.execute_reply":"2025-06-11T04:51:39.608282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"##Summary\n\ndisplay(df.head(), df.info(), df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:39.610503Z","iopub.execute_input":"2025-06-11T04:51:39.610769Z","iopub.status.idle":"2025-06-11T04:51:39.666142Z","shell.execute_reply.started":"2025-06-11T04:51:39.610749Z","shell.execute_reply":"2025-06-11T04:51:39.665230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##looking at this missing values in the data\nplt.figure(figsize=(12, 6))\nsns.heatmap(df.isnull(), cbar=False, cmap='YlGnBu')\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n\nprint(\"Missing Count:\\n\", df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:39.667002Z","iopub.execute_input":"2025-06-11T04:51:39.667246Z","iopub.status.idle":"2025-06-11T04:51:40.137284Z","shell.execute_reply.started":"2025-06-11T04:51:39.667227Z","shell.execute_reply":"2025-06-11T04:51:40.136465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(x='noshow', data=df)\nplt.title(\"Target Class Distribution\")\nplt.show()\n\ndf['noshow'].value_counts(normalize=True).plot(kind='pie', autopct='%1.1f%%', startangle=90)\nplt.title(\"No-Show Distribution\")\nplt.ylabel(\"\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:40.139053Z","iopub.execute_input":"2025-06-11T04:51:40.139281Z","iopub.status.idle":"2025-06-11T04:51:40.355129Z","shell.execute_reply.started":"2025-06-11T04:51:40.139264Z","shell.execute_reply":"2025-06-11T04:51:40.354242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 11.5% no shows. This is imbalanced data set, we will apply sampling techniques during modeling stage","metadata":{}},{"cell_type":"code","source":"categorical_cols = df.select_dtypes(include='object').columns.tolist()\nfor col in categorical_cols:\n    plt.figure(figsize=(10, 4))\n    sns.countplot(data=df, x=col, hue='noshow', order=df[col].value_counts().index)\n    plt.title(f'{col} vs No-Show')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:40.355918Z","iopub.execute_input":"2025-06-11T04:51:40.356153Z","iopub.status.idle":"2025-06-11T04:51:42.044054Z","shell.execute_reply.started":"2025-06-11T04:51:40.356137Z","shell.execute_reply":"2025-06-11T04:51:42.043058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Takeaways from the Plots\n\n\n* No-shows occur in all time slots but slightly more concentrated in afternoon and morning.\n\n* Opportunity: Flag customers booking during afternoon/morning with higher caution\n\n* No-shows are evenly distributed across weekdays\n\n* Sundays and Mondays have low overall booking volume\n\n* No standout high-risk days, but weekend bookings don’t reduce no-shows\n\n* Most no-shows are associated with STYLE category, Fewer no-shows for COLOR and MISC\n\n* Consider weighting book_category=STYLE as a higher-risk group\n\n* Staff 'B' and 'BECKY' have the highest number of bookings and no-shows\n\n* However, this might reflect volume, not necessarily poor performance\n\n* Normalizing by total appointments per staff would give clearer insights\n","metadata":{}},{"cell_type":"code","source":"numerical_cols = df.select_dtypes(include=['int64', 'float64']).drop(columns='noshow').columns\n\n# Histograms\ndf[numerical_cols].hist(figsize=(15, 10), bins=30)\nplt.suptitle(\"Numerical Feature Distributions\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:42.045005Z","iopub.execute_input":"2025-06-11T04:51:42.045294Z","iopub.status.idle":"2025-06-11T04:51:44.391246Z","shell.execute_reply.started":"2025-06-11T04:51:42.045274Z","shell.execute_reply":"2025-06-11T04:51:44.390481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nsns.heatmap(df.corr(numeric_only=True), annot=True, fmt=\".2f\", cmap='coolwarm')\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:44.392383Z","iopub.execute_input":"2025-06-11T04:51:44.392700Z","iopub.status.idle":"2025-06-11T04:51:45.011387Z","shell.execute_reply.started":"2025-06-11T04:51:44.392671Z","shell.execute_reply":"2025-06-11T04:51:45.010540Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Observations:\n\n* noshow is weakly correlated with all numerical features (mostly < ±0.2).\n* last_cumnoshow (0.21) has the strongest positive correlation with no-show.\n* High multicollinearity exists among:last_cumrev, last_cumbook, last_cumstyle, last_cumcolor, last_cumprod — all > 0.9\n* Consider dropping or combining these\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(df, hue='noshow', vars=['last_receipt_tot', 'recency', 'last_cumrev', 'last_cumcancel'], plot_kws={'alpha': 0.3})\nplt.suptitle(\"Pairwise Plots\", y=1.02)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:45.012445Z","iopub.execute_input":"2025-06-11T04:51:45.012839Z","iopub.status.idle":"2025-06-11T04:51:50.485653Z","shell.execute_reply.started":"2025-06-11T04:51:45.012781Z","shell.execute_reply":"2025-06-11T04:51:50.484954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nInsights from Pairplot:\n* recency and last_cumcancel show some visual separability: higher recency and more past cancels appear slightly more common in no-shows.\n* last_receipt_tot and last_cumrev are skewed (heavy on the left) — consider log-transforming.There is no clear linear separation ","metadata":{}},{"cell_type":"markdown","source":"# Step 3: Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"##  Handle Missing Values","metadata":{}},{"cell_type":"code","source":"# Fill missing categorical with 'Unknown'\nfor col in ['book_tod', 'last_category', 'last_staff', 'last_dow', 'last_tod']:\n    df[col].fillna('Unknown', inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:50.486853Z","iopub.execute_input":"2025-06-11T04:51:50.487127Z","iopub.status.idle":"2025-06-11T04:51:50.496243Z","shell.execute_reply.started":"2025-06-11T04:51:50.487105Z","shell.execute_reply":"2025-06-11T04:51:50.495463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Transformations","metadata":{}},{"cell_type":"code","source":"#Create same_staff and same_category flags\ndf['same_staff'] = (df['book_staff'] == df['last_staff']).astype(int)\ndf['same_category'] = (df['book_category'] == df['last_category']).astype(int)\n\n#Time-of-Day Buckets\n\ndef tod_bucket(t):\n    if t == 'Unknown': return 'Unknown'\n    try:\n        hour = int(t.split(':')[0])\n        if hour < 12: return 'Morning'\n        elif hour < 17: return 'Afternoon'\n        else: return 'Evening'\n    except:\n        return 'Unknown'\n\ndf['book_tod_bucket'] = df['book_tod'].apply(tod_bucket)\ndf['last_tod_bucket'] = df['last_tod'].apply(tod_bucket)\n\n#Receipt per Service (stabilized with +1)\n\ndf['receipt_per_service'] = df['last_receipt_tot'] / (df['last_day_services'] + 1)\n\n#Past No-Show Rate\n\ndf['noshow_rate'] = df['last_cumnoshow'] / (df['last_cumbook'] + 1)\n\n#Remove Highly Correlated Features\n\ndf.drop(columns=['last_cumbook', 'last_cumstyle', 'last_cumcolor', 'last_cumprod'], inplace=True)\n\n#Encode Categorical Features\n\ncategorical_cols = [\n    'book_dow', 'book_category', 'book_staff',\n    'last_category', 'last_staff', 'last_dow',\n    'book_tod_bucket', 'last_tod_bucket'\n]\n\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\n#Drop Now-Unnecessary Columns\n\ndf_encoded.drop(columns=['book_tod', 'last_tod'], inplace=True)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:50.498770Z","iopub.execute_input":"2025-06-11T04:51:50.499028Z","iopub.status.idle":"2025-06-11T04:51:50.531742Z","shell.execute_reply.started":"2025-06-11T04:51:50.499010Z","shell.execute_reply":"2025-06-11T04:51:50.530845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 4: Select and train a model","metadata":{}},{"cell_type":"code","source":"#Train-Test Split\nX = df_encoded.drop(columns='noshow')\ny = df_encoded['noshow']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.2, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:50.532722Z","iopub.execute_input":"2025-06-11T04:51:50.533423Z","iopub.status.idle":"2025-06-11T04:51:50.543525Z","shell.execute_reply.started":"2025-06-11T04:51:50.533387Z","shell.execute_reply":"2025-06-11T04:51:50.542748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train 3 Models\n\n* Logistic Regression (class_weight='balanced')\n* Random Forest (class_weight='balanced')\n* XGBoost (scale_pos_weight=imbalance_ratio)\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Step 1: Train 3 models with class_weight or scale_pos_weight handling imbalance\n\n# Calculate imbalance ratio for XGBoost\nimbalance_ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n\n# Logistic Regression (requires scaling)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nlogreg.fit(X_train_scaled, y_train)\ny_pred_logreg = logreg.predict(X_test_scaled)\ny_proba_logreg = logreg.predict_proba(X_test_scaled)[:, 1]\n\n# Random Forest\nrf = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=200)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\ny_proba_rf = rf.predict_proba(X_test)[:, 1]\n\n# XGBoost\nxgb = XGBClassifier(scale_pos_weight=imbalance_ratio, use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\ny_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n\n# Define evaluation function\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n    pr_auc = auc(recall, precision)\n    return {\n        'Model': name,\n        'F1': f1_score(y_true, y_pred),\n        'ROC AUC': roc_auc_score(y_true, y_proba),\n        'PR AUC': pr_auc,\n        'Precision': precision_score(y_true, y_pred),\n        'Recall': recall_score(y_true, y_pred)\n    }\n\n# Collect metrics\nresults = [\n    evaluate_model('Logistic Regression', y_test, y_pred_logreg, y_proba_logreg),\n    evaluate_model('Random Forest', y_test, y_pred_rf, y_proba_rf),\n    evaluate_model('XGBoost', y_test, y_pred_xgb, y_proba_xgb)\n]\n\nimport pandas as pd\nresults_df = pd.DataFrame(results)\n\n\n# Also returning confusion matrices\nconfusion_matrices = {\n    \"Logistic Regression\": confusion_matrix(y_test, y_pred_logreg),\n    \"Random Forest\": confusion_matrix(y_test, y_pred_rf),\n    \"XGBoost\": confusion_matrix(y_test, y_pred_xgb)\n}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:50.544246Z","iopub.execute_input":"2025-06-11T04:51:50.544448Z","iopub.status.idle":"2025-06-11T04:51:51.718242Z","shell.execute_reply.started":"2025-06-11T04:51:50.544432Z","shell.execute_reply":"2025-06-11T04:51:51.717466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(y_true, y_pred, y_proba):\n    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n    return {\n        \"F1\": f1_score(y_true, y_pred),\n        \"ROC AUC\": roc_auc_score(y_true, y_proba),\n        \"PR AUC\": auc(rec, prec),\n        \"Precision\": precision_score(y_true, y_pred),\n        \"Recall\": recall_score(y_true, y_pred)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:51.719116Z","iopub.execute_input":"2025-06-11T04:51:51.719439Z","iopub.status.idle":"2025-06-11T04:51:51.724688Z","shell.execute_reply.started":"2025-06-11T04:51:51.719411Z","shell.execute_reply":"2025-06-11T04:51:51.723974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def metrics_from_confusion_matrix(name, cm):\n    TN, FP, FN, TP = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n    \n    precision = TP / (TP + FP) if (TP + FP) else 0\n    recall = TP / (TP + FN) if (TP + FN) else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n    \n    return {\n        \"Model\": name,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1 Score\": f1\n    }\n\n# Compute metrics for all models\nmetrics_summary = [metrics_from_confusion_matrix(name, cm) for name, cm in confusion_matrices.items()]\nmetrics_df = pd.DataFrame(metrics_summary)\nmetrics_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:51.725572Z","iopub.execute_input":"2025-06-11T04:51:51.725820Z","iopub.status.idle":"2025-06-11T04:51:51.751179Z","shell.execute_reply.started":"2025-06-11T04:51:51.725779Z","shell.execute_reply":"2025-06-11T04:51:51.750195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Applying SMOTE","metadata":{}},{"cell_type":"code","source":"\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\ndf_encoded.drop(columns=['book_tod', 'last_tod'], inplace=True)\n\n# Split features/target\nX = df_encoded.drop(columns='noshow')\ny = df_encoded['noshow']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.2, random_state=42\n)\n\n# Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Standard scaling for logistic regression\nscaler = StandardScaler()\nX_resampled_scaled = scaler.fit_transform(X_resampled)\nX_test_scaled = scaler.transform(X_test)\n\n# Train models on SMOTE-resampled data\nimbalance_ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n\nlogreg_smote = LogisticRegression(max_iter=1000, random_state=42)\nlogreg_smote.fit(X_resampled_scaled, y_resampled)\ny_pred_logreg = logreg_smote.predict(X_test_scaled)\ny_proba_logreg = logreg_smote.predict_proba(X_test_scaled)[:, 1]\n\nrf_smote = RandomForestClassifier(random_state=42, n_estimators=200)\nrf_smote.fit(X_resampled, y_resampled)\ny_pred_rf = rf_smote.predict(X_test)\ny_proba_rf = rf_smote.predict_proba(X_test)[:, 1]\n\nxgb_smote = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_smote.fit(X_resampled, y_resampled)\ny_pred_xgb = xgb_smote.predict(X_test)\ny_proba_xgb = xgb_smote.predict_proba(X_test)[:, 1]\n\n# Evaluation function\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n    pr_auc = auc(recall, precision)\n    return {\n        'Model': name,\n        'F1': f1_score(y_true, y_pred),\n        'ROC AUC': roc_auc_score(y_true, y_proba),\n        'PR AUC': pr_auc,\n        'Precision': precision_score(y_true, y_pred),\n        'Recall': recall_score(y_true, y_pred)\n    }\n\n# Collect metrics\nresults_smote = [\n    evaluate_model('Logistic Regression (SMOTE)', y_test, y_pred_logreg, y_proba_logreg),\n    evaluate_model('Random Forest (SMOTE)', y_test, y_pred_rf, y_proba_rf),\n    evaluate_model('XGBoost (SMOTE)', y_test, y_pred_xgb, y_proba_xgb)\n]\n\nresults_smote_df = pd.DataFrame(results_smote)\nconfusion_matrices = {\n    \"Logistic Regression\": confusion_matrix(y_test, y_pred_logreg),\n    \"Random Forest\": confusion_matrix(y_test, y_pred_rf),\n    \"XGBoost\": confusion_matrix(y_test, y_pred_xgb)\n}\n\nmetrics_summary = [metrics_from_confusion_matrix(name, cm) for name, cm in confusion_matrices.items()]\nmetrics_df = pd.DataFrame(metrics_summary)\nmetrics_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:51.752112Z","iopub.execute_input":"2025-06-11T04:51:51.752366Z","iopub.status.idle":"2025-06-11T04:51:53.162770Z","shell.execute_reply.started":"2025-06-11T04:51:51.752348Z","shell.execute_reply":"2025-06-11T04:51:53.162088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"* Logistic Regression wins based on:\n\n    * Highest Recall (0.489): catches more no-shows\n    \n    * Best F1 Score (0.30): good balance of precision & recall\n\n* Logistic Regression without SMOTE actually had higher recall (0.489 vs 0.400)\n\n* If the goal is maximum Recall (catching no-shows):\n    * Stick with Logistic Regression (No SMOTE) + class_weight='balanced'","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Fine-tune the model","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_recall_curve, recall_score, precision_score, f1_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Preprocessing\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 2. Define logistic regression model\nlogreg_orig = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nlogreg_orig.fit(X_train_scaled, y_train)\n\n# 3. Predict probabilities\ny_proba = logreg_orig.predict_proba(X_test_scaled)[:, 1]\n\n# 4. Tune threshold to get ~80% recall\nthresholds = np.arange(0.0, 1.01, 0.01)\nbest_threshold = 0\nbest_metrics = {}\n\nfor t in thresholds:\n    y_pred_thresh = (y_proba >= t).astype(int)\n    recall = recall_score(y_test, y_pred_thresh)\n    \n    if recall >= 0.80:\n        precision = precision_score(y_test, y_pred_thresh)\n        f1 = f1_score(y_test, y_pred_thresh)\n        best_threshold = t\n        best_metrics = {\n            \"Threshold\": t,\n            \"Recall\": recall,\n            \"Precision\": precision,\n            \"F1 Score\": f1\n        }\n        break  # stop at first threshold that meets recall ≥ 0.80\n\nprint(\"Best Threshold for ≥80% Recall:\")\nprint(best_metrics)\n\n# Optional: visualize precision-recall tradeoff\nprec, rec, thresh = precision_recall_curve(y_test, y_proba)\nplt.plot(thresh, rec[:-1], label='Recall')\nplt.plot(thresh, prec[:-1], label='Precision')\nplt.axvline(best_threshold, color='red', linestyle='--', label=f'Threshold {best_threshold:.2f}')\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Threshold vs Precision/Recall\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:53.163595Z","iopub.execute_input":"2025-06-11T04:51:53.163830Z","iopub.status.idle":"2025-06-11T04:51:53.882790Z","shell.execute_reply.started":"2025-06-11T04:51:53.163791Z","shell.execute_reply":"2025-06-11T04:51:53.881885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To target Recall ≈ 0.80 with better balance:\n\n* Look for the threshold where Recall drops to just above 0.80\n* Record associated Precision and F1\n* Use that threshold in production to flag high-risk no-shows\n","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear'],  # 'liblinear' supports both l1 and l2\n    'class_weight': ['balanced']\n}\n\ngrid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5,\n                    scoring='recall', n_jobs=-1, verbose=1)\n\ngrid.fit(X_train_scaled, y_train)\n\nprint(\"Best Parameters:\", grid.best_params_)\nprint(\"Best Recall:\", grid.best_score_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:53.883666Z","iopub.execute_input":"2025-06-11T04:51:53.883971Z","iopub.status.idle":"2025-06-11T04:51:56.279407Z","shell.execute_reply.started":"2025-06-11T04:51:53.883949Z","shell.execute_reply":"2025-06-11T04:51:56.278478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the best estimator from GridSearchCV\nbest_model = grid.best_estimator_\n\n# Predict probabilities\ny_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n\n# Threshold tuning if you want Recall ≥ 0.80\nthreshold = 0.3\ny_pred_best = (y_proba_best >= threshold).astype(int)\n\n# Final Evaluation\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"Classification Report (Threshold =\", threshold, \")\")\nprint(classification_report(y_test, y_pred_best))\n\nprint(\"\\nConfusion Matrix\")\nprint(confusion_matrix(y_test, y_pred_best))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:51:56.280331Z","iopub.execute_input":"2025-06-11T04:51:56.280573Z","iopub.status.idle":"2025-06-11T04:51:56.299926Z","shell.execute_reply.started":"2025-06-11T04:51:56.280555Z","shell.execute_reply":"2025-06-11T04:51:56.298937Z"}},"outputs":[],"execution_count":null}]}